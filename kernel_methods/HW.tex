\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{rotate}
\usepackage{amsfonts}

\input{../../divers/header.tex}
\newcounter{exo}
\newenvironment{exo}[1]{\refstepcounter{exo}\vspace{0.5cm}{\bfseries Exercice \theexo. #1\\ }}{\par\vspace{0.5cm}}


\begin{document}

\title{MVA "Kernel methods in machine learning"\\Homework}
\author{Julien Mairal and Jean-Philippe Vert}
\date{}
\maketitle
\begin{center}
\fbox{\parbox{10cm}{\centering
Upload your answers (in PDF) to:\\
 \url{http://tiny.cc/dpxnjz} \\
 before February 26, 2020, 1pm (Paris time).}}
\end{center}
 
  \begin{exo}{Kernels}
Show that the following kernels are positive definite:
\begin{enumerate}
\item Let $\Xcal$ be a set and $f,g:\Xcal\rightarrow\RR_+$ two non-negative functions:
$$
\forall x,y\in\Xcal\quad K_4(x,y) = \min(f(x)g(y) , f(y)g(x))
$$
\item Given a non-empty finite set $E$, on $\Xcal=\Pcal(E) = \cbr{A\,:\,A\subset E}$:
$$
\forall A,B \subset E\,,\quad K\br{A,B} = \frac{\abs{A \cap B}}{\abs{A \cup B}}\,,
$$ 
where $\abs{F}$ denotes the cardinality of $F$, and with the convention $\frac{0}{0}=0$.
\end{enumerate}
\end{exo}

\begin{exo}{Kernels encoding equivalence classes.}
Consider a similarity measure $K: \Xcal \times \Xcal \to \{0,1\}$ with $K(x,x)=1$ for all $x$ in $\Xcal$. Prove that $K$ is p.d. if and only if, for all $x, x', x''$ in $\Xcal$,  
\begin{itemize}
\item  $K(x,x')=1 \Leftrightarrow K(x',x)=1$, and
\item  $K(x,x')=K(x',x'') = 1 \Rightarrow K(x,x'')=1$.
\end{itemize}
\end{exo}

\begin{exo}{COCO}
Given two sets of real numbers $X=(x_1,\ldots,x_n)\in\RR^n$ and $Y=(y_1,\ldots,y_n)\in\RR^n$, the covariance between $X$ and $Y$ is defined as
$$
cov_n(X,Y) = \Eb_n (XY) - \Eb_n (X) \Eb_n (Y)\,,
$$
where $\Eb_n(U) = (\sum_{i=1}^n u_i)/n$. The covariance is useful to detect linear relationships between $X$ and $Y$. In order to extend this measure to potential nonlinear relationships between $X$ and $Y$, we consider the following criterion:
$$
C^K_n(X,Y) = \max_{f,g\in\Bcal_K} cov_n(f(X),g(Y))\,,
$$
where $K$ is a positive definite kernel on $\RR$, $\Bcal_K$ is the unit ball of the RKHS of $K$, and $f(U) = \br{f(u_1),\ldots,f(u_n)}$ for a vector $U=(u_1,\ldots,u_n)$.
\begin{enumerate}
\item Express simply $C^K_n(X,Y)$ for the linear kernel $K(a,b)=ab$.
\item For a general kernel $K$, express $C^K_n(X,Y)$ in terms of the Gram matrices of $X$ and $Y$.
   \end{enumerate}
\end{exo}

\begin{exo}{Dual coordinate ascent algorithms for SVMs}
We recall the primal formulation of SVMs seen in the class (slide 148).
   $$
   \min_{f \in \Hcal} \frac{1}{n} \sum_{i=1}^n \max(0,1-y_i f(\x_i)) + \lambda \|f\|_{\Hcal}^2,
   $$
   and its dual formulation (slide 158)
   $$
   \max_{\alphab \in \Real^n} 2 \alphab^\top \y - \alphab^\top \Kb\alphab ~~~\text{such that}~~~ 0 \leq y_i \alpha_i \leq \frac{1}{2\lambda n},~~\text{for all}~i.
   $$
\begin{enumerate}
\item    The coordinate ascent method consists of iteratively optimizing with respect to one variable, while fixing the other ones.
   Assuming that you want to maximize the dual by following this approach. Find (and justify) the update rule for $\alpha_j$.
\item   Consider now the primal formulation of SVMs with intercept
   $$
   \min_{f \in \Hcal, b \in \Real} \frac{1}{n} \sum_{i=1}^n \max(0,1-y_i (f(\x_i)+b)) + \lambda \|f\|_{\Hcal}^2,
   $$
   Can we still apply the representer theorem? Why?  Derive the corresponding
   dual formulation by using Lagrangian duality.  Can we apply the coordinate
   ascent method to this dual? If yes, what are the update rules?
\item
   Consider a coordinate ascent method to this dual that consists of updating
   two variables~$(\alpha_i,\alpha_j)$ at a time (while fixing the $n-2$ other
   variables). What are the update rules for these two variables?
   \end{enumerate}
\end{exo}



 \begin{exo}{Duality}
Let $(x_1,y_1), \ldots, (x_n,y_n)$ a training set of examples where $x_i\in\Xcal$, a space endowed with a positive definite kernel $K$, and $y_i\in\cbr{-1,1}$, for $i=1,\ldots,n$. $\Hcal_K$ denotes the RKHS of the kernel $K$. We want to learn a function $f:\Xcal\mapsto\RR$ by solving the following optimization problem:
\begin{equation}\label{eq1}
\min_{f \in \Hcal_K} \frac{1}{n} \sum_{i=1}^n \ell_{y_i}\br{f(x_i)} \quad\text{such that}\quad \nm{f}_{\Hcal_K}\leq B \,,
\end{equation}
where $\ell_y$ is a convex loss functions (for $y\in\cbr{-1,1}$) and $B>0$ is a parameter.
\begin{enumerate}
\item Show that there exists $\lambda\geq 0$ such that the solution to problem (\ref{eq1}) can be found be solving the following problem:
\begin{equation}\label{eq2}
\min_{\alpha\in\RR^n} R(K\alpha) +\lambda \alpha^\top K \alpha\,,
\end{equation}
where $K$ is the $n\times n$ Gram matrix and $R:\RR^n\mapsto \RR$ should be explicited.
\item Compute the Fenchel-Legendre transform\footnote{For any function $f:\RR^N \mapsto \RR$, the \emph{Fenchel-Legendre transform} (or \emph{convex conjugate}) of $f$ is the function $f^*:\RR^N \mapsto \RR$ defined by 
$$
f^*(u) = \sup_{x\in\RR^N} x^\top u - f(x)\,.
$$
} $R^*$ of $R$ in terms of the Fenchel-Legendre transform $\ell_y^*$ of $\ell_y$.
\item Adding the slack variable $u=K\alpha$, the problem (\ref{eq1}) can be rewritten as a constrained optimization problem:
\begin{equation}\label{eq3}
\min_{\alpha\in\RR^n, u\in\RR^n} R(u) +\lambda \alpha^\top K \alpha\quad\text{such that}\quad u=K\alpha\,.
\end{equation}
Express the dual problem of (\ref{eq3}) in terms of $R^*$, and explain how a solution to (\ref{eq3}) can be found from a solution to the dual problem.
\item Explicit the dual problem for the logistic and squared hinge losses: $$\ell_y(u) = \log(1+e^{-yu})\,.$$ $$\ell_y(u) = \max(0,1-yu)^2\,.$$
\end{enumerate}
\end{exo}\end{document}