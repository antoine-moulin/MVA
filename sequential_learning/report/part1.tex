\section*{Part 1. Link between online learning and game theory}

We consider the sequential version of a two-player zero-sum games between a player and an adversary. 
\begin{setting}[ht!]
\begin{center}
\fbox{
    \begin{minipage}{.95\textwidth}
		Let $L \in [-1,1]^{M \times N}$ be a loss matrix.  \\[2pt]
        At each round $t=1,\dots,T$
        \begin{itemize}
            \item The player choose a distribution $p_t \in \Delta_{M} := \{p \in [0,1]^M, \sum_{i=1}^M p_i = 1\}$
            \item The adversary chooses a distribution $q_t \in \Delta_N$
            \item The actions of both players are sampled $i_t \sim p_t$ and $j_t \sim q_t$
            \item The player incurs the loss $L(i_t,j_t)$ and the adversary the loss $-L(i_t,j_t)$. 
        \end{itemize}
    \end{minipage}}
\end{center}

\caption{Setting of a sequential two-player zero sum game}
\label{fig:setting}
\end{setting}

\begin{enumerate}
	\item Recall $M$, $N$ and a loss matrix $L \in [-1,1]^{M\times N}$ that corresponds to the game ``Rock paper scissors''\footnote{This is a common game where two players choose one of 3 options: (Rock, Paper, Scissors). The winner is decided according to the following: Rock crushes scissors, Paper covers Rock, Scissors cuts paper}.
	
    \begin{solution}
For the game "Rock paper scissors", the player and the adversary have three possible actions, so:

\begin{equation*}
        \boxed{N=M=3}
\end{equation*}

and the loss is (in a basis "Rock paper scissors"):

\begin{equation*}
            \boxed{L = \begin{pmatrix}
       0 & 1 & -1 \\
       -1 & 0 & 1 \\
       1 & -1 & 0
    \end{pmatrix}}
        \end{equation*}
    \end{solution}
\end{enumerate}

\paragraph{Full information feedback}
In this part, we assume that both players know the matrix $L$ in advance and can compute $L(i,j)$ for any $(i,j)$. 

\begin{enumerate}[resume]
	\item \emph{Implementation of \lstinline{EWA}.} 
	\begin{enumerate}[label=(\alph*)] 
		\item In order to implement the exponential weight algorithm, you need a way to sample from the exponential weight distribution. Implement the function \lstinline{rand_weighted} that takes as input a probability vector $p \in \Delta_M$ and uses a single call to \lstinline{rand()} to return $X \in [M]$ with $P(X = i) = p_i$.
		
	    \begin{solution}
    See code.
        \end{solution}


		\item Define a function \lstinline{EWA_update} that takes as input a vector $p_t \in \Delta_M$ and a loss vector $\ell_t \in [-1,1]^M$ and return the updated vector $p_{t+1} \in \Delta_M$ defined for all $i \in [M]$ by
		\begin{equation*}
			p_{t+1}(i) = \frac{p_t(i) \exp(-\eta \ell_t(i))}{\sum_{j=1}^M p_t(j) \exp(-\eta \ell_t(j))} \,.
		\end{equation*}
		
	    \begin{solution}
    See code.
        \end{solution}
	\end{enumerate}

	\item \emph{Simulation against a fixed adversary.} Consider the game ``Rock paper scissors'' and assume that the adversary chooses $q_t = (1/2, 1/4, 1/4)$ and samples $j_t \sim q_t$ for all rounds $t \geq 1$. 
	\begin{enumerate}[label=(\alph*)]
		\item What is the loss $\ell_t(i)$ incurred by the player if he chooses action $i$ at time $t$? Simulate an instance of the game for $t = 1,\dots, T= 100$ for $\eta = 1$.
		
	    \begin{solution}
    The loss incurred by the player if he chooses action i at time t is:
    
    \begin{equation*}
        \boxed{\ell_t(i)=L(i,j_t)}
    \end{equation*}
        \end{solution}
		
		
 		\item Plot the evolution of the weight vectors $p_1,p_2,\dots,p_T$. What seems to be the best strategy against this adversary?
 		
	    \begin{solution}
\begin{figure}[h!]
\begin{center}
\includegraphics[height=170 pt]{image1/q3b_weights.png} 
\end{center}
\caption{Evolution of the weight vectors from the EWA algorithm for $\eta = 1$ and $T = 100$, as a function of $t$.}
\end{figure}
From this figure, we can conclude that the best strategy against this adversary is to play the action 1 all the time. It is indeed quite coherent with the game, since the action 1 (paper) wins over the action 0 (rock), which is the most played since the adversary chooses it half of the time ($q_t = (1/2,1/4,1/4)$).
        \end{solution}
 		
 		
 		\item Plot the average loss $\bar \ell_t = \frac{1}{t} \sum_{s=1}^t \ell(i_s,j_s)$ as a function of $t$.
 		
	    \begin{solution}
	    \begin{figure}[h!]
\begin{center}
\includegraphics[height=170 pt]{image1/q3c_average_loss.png} 
\end{center}
\caption{Average loss for EWA algorithm for $\eta = 1$ and $T = 100$, as a function of $t$.}
\end{figure}
We notice that the average loss tends to be negative, meaning the player is winning in the long run. It shows that the EWA strategy is working against a non-adaptative adversary. The limit seems to be $-0.2$, which corresponds to the expected loss when the probability distributions are $p = \br{0, 1, 0}$ and $q = \br{1/2, 1/4, 1/4}$.
        \end{solution}
 		
 		
 		\item Plot the cumulative regret.

	    \begin{solution}
Let us plot the cumulative regret compared to: 
\begin{itemize}
    \item The optimal bound (for the optimal $\eta$): $f(t)=2\sqrt{tlog(3)}$.
    \item The actual bound: $f(t) = \eta t + \frac{log(3)}{\eta}$.
\end{itemize}
\begin{figure}[h!]
\centering
\begin{subfigure}{.47\textwidth}
  \centering
  \includegraphics[height=150pt]{image1/q3d_regrets.png}
  \caption{Cumulative regret of the EWA algorithm for $\eta = 1$ and $T = 100$, as a function of $t$, compared with the real bound and its optimal one.}
  \label{fig:sub1}
\end{subfigure}%
\hspace{2mm}
\begin{subfigure}{.47\textwidth}
  \centering
  \includegraphics[height=150pt]{image1/q3d_regrets_bis.png}
  \caption{Cumulative regret of the EWA algorithm for $\eta = 1$ and $T=100$, as a function of t, compared with the optimal bound.}
  \label{fig:sub2}
\end{subfigure}%
\caption{Cumulative regret of the EWA.}
\label{fig:cum-regrets}
\end{figure}
The cumulative regret is below the actual and the optimal bound, even with a non optimal value of $\eta$.
        \end{solution}
 		
 		
 		\item To see if the algorithm is stable, repeat the simulation $n = 10$ times and plot the average loss $(\bar \ell_t)_{t\geq 1}$ obtained in average, in maximum and in minimum over the $n$ simulations.
 		
	    \begin{solution}
	    The plot is given in figure \ref{fig:loss-measures} and shows that the algorithm is stable: the minimum and the maximum loss are both concentrating around the average loss, what indicates a low variance average loss when t increases.
	\begin{figure}   
    \begin{center}
    \includegraphics[height=150 pt]{image1/q3e_average_simulations.png}
    \end{center}
    \caption{Average loss for EWA over 10 simulations for $\eta = 1$ and $T=100$, as a function of $t$.}
    \label{fig:loss-measures}
    \end{figure}
        \end{solution}
 	
 		\item Repeat one simulation for different values of learning rates $\eta \in \{0.01,0.05,0.1,0.5,1\}$ and plot the final regret as a function of $\eta$. What are the best $\eta$ in practice and in theory.
 		
	    \begin{solution}
    \begin{figure}[h!]  
    \begin{center}
    \includegraphics[height=170 pt]{image1/q3f_eta_variations_100.png}
    \end{center}
    \caption{Average loss for EWA for different values of $eta$ and $T=100$, as a function of $t$.}
    \end{figure}
    
    \begin{figure}[h!]   
    \begin{center}
    \includegraphics[height=170 pt]{image1/q3f_eta_variations_1000.png}
    \end{center}
    \caption{Average loss for EWA for different values of $eta$ and $T=1000$, as a function of $t$.}
    \end{figure}
        \end{solution}

The best values for $\eta$ are obtained for $\eta \in [0.2,1]$. We performed the simulation for T=100, and for T=1000, since the final regrets are used to compare $\eta$. In theory, the best value for $\eta$ is achieved with: 
\begin{equation*}
    \eta = \sqrt{\frac{log(M)}{T}}
\end{equation*}

Thus, $\eta \approx 0.03$ if $T=1000$, and $\eta \approx 0.1$ for $T=100$. However, we reach better results for higher values of $\eta$. Indeed, the plot in figure \ref{fig:cum-regrets} showed that the optimal bound is not tight. We could explain this phenomenon with the quite simple structure of the problem. Lower values of $\eta$ allow to explore more first. But here, since the action is not adaptative, higher values of $\eta$ only allows to converge quicker to action 1 (question (3.b)), and keep the loss slow in the first plays.
	\end{enumerate}

	\item \emph{Simulation against an adaptive adversary.} Repeat the simulation of question 3) when the adversary is also playing EWA with learning parameters $\eta = 0.05$.  

	\begin{enumerate}[label=(\alph*)]
		\item Plot $\frac{1}{t} \sum_{s=1}^t \ell(i_s,j_s)$ as a function of $t$. 
		
	    \begin{solution}
    In this question, the adversary becomes adaptative with EWA updates and $\eta_2=0.05$, the player still follows EWA updates with $\eta_1 = 0.05$.
     \begin{figure}[h!]  
    \begin{center}
    \includegraphics[height=150 pt]{image1/q4a_average_loss.png}
    \end{center}
    \caption{Average loss for an adaptative EWA adversary as a function of $t$, with $T = 10000$, $\eta_1 = 0.05$, and $\eta_2 = 0.05$.}
    \end{figure}
    
    The average loss converges to $0$. This is coherent with the course (if the adversary follows a regret minimization algorithm, the average loss converges almost surely). Besides, since the losses are symmetric for the player and its adversary, the value of the limit is also coherent.
        \end{solution}

		\smallskip
		It is possible to show that if both players  play according to a regret minimizing strategy the cumulative
loss of the player converges to the value of the game
		\begin{equation*}
			V = \min_{p \in \Delta_M} \max_{q \in 
\Delta_q} \ p^\top L q \,.
		\end{equation*}
		\item Define $\bar p_t = \frac{1}{t} \sum_{s=1}^t p_s$. Plot in log scale $\|\bar p_t - (1/3,1/3,1/3)\|_2$ as a function of $t=1,\dots,10\,000$. 
		
	    \begin{solution}
\begin{figure}
\begin{center}
\includegraphics[height=170 pt]{image1/EWA_norm.png} 
\end{center}
\caption{Plot of $||\bar{p_t}-(1/3,1/3,1/3)||_2$ and $||\bar{q_t}-(1/3,1/3,1/3)||_2$, for $\eta=0.05$ and $T=10000$}
\end{figure}

The norm $\|\bar p_t - (1/3,1/3,1/3)\|_2$ converges to $0$, and oscillates while decreasing, and so does $\|\bar q_t - (1/3,1/3,1/3)\|_2$. This is again coherent with the Nash equilibrium : both player follows a minimization regret algorithm, and thus tend to have a uniform action.
        \end{solution}

		\smallskip
		It is possible to show that $(\bar p_t, \bar q_t)_{t\geq 1}$ converges almost surely to a Nash equilibrium of the game. This means that if $p\times q$ is a Nash equilibrium, none of the players should change is strategy if the other player does not change hers. 
	\end{enumerate}
\end{enumerate}

\paragraph{Bandit feedback}
Now, we assume that the players do not know the game in advance but only observe the performance $L(i_t,j_t)$ (that we assume here to be in $[0,1]$) of the actions played at time $t$. They need to learn the game and adapt to the adversary as one goes along.

\begin{enumerate}[resume]
    \item \emph{Implementation of \lstinline{EXP3}}. Since both players are symmetric, we focus on the first player. 
    \begin{enumerate}[label=(\alph*)]
    	\item Implement the function \lstinline{estimated_loss} that takes as input the action $i_t \in [M]$ played at round $t\geq 1$ and the loss $L(i_t,j_t)$ suffered by the player and return the vector of estimated loss $\widehat \ell_t \in \R_+^M$ used by \lstinline{EXP3}.
	
	    \begin{solution}
    See the code.
        \end{solution}
	
	
	    \item Implement the function \lstinline{EXP3_update} that takes as input a vector $p_t \in \Delta_M$, the action $i_t \in [M]$ played by the player and the loss $L(i_t,j_t)$ and return the updated weight vector $p_{t+1} \in \Delta_M$.
	    
	    \begin{solution}
    See the code.
        \end{solution}
    \end{enumerate}

    \item Repeat Questions 3.a) to 3.f) with \lstinline{EXP3} instead of \lstinline{EWA}.
    
    \begin{solution}
    When using \lstinline{EXP3} instead of \lstinline{EWA}, we observe that even though the algorithm takes more time to learn the optimal strategy, we obtain similar results for questions 3.a) to 3.f). Indeed, although the information is now incomplete, the game is simple enough so the algorithm can learn to counter a fixed adversary and have a low regret. To see the plots, one can just run the script \lstinline{main.py}. Here are some of them in figure \ref{fig:cum-regrets}.
    
    \begin{figure}[!h]
\centering
\begin{subfigure}{.28\textwidth}
  \centering
  \includegraphics[height=100pt]{image1/EXP3/EXP3_average_loss.png}
  \caption{Cumulative regret of EXP3, for $\eta = 1, T = 1000$, as a function of $t$, compared with the real and optimal bounds.}
  \label{fig:sub1}
\end{subfigure}%
\hspace{.5mm}
\begin{subfigure}{.28\textwidth}
  \centering
  \includegraphics[height=100pt]{image1/EXP3/EXP3_proba.png}
  \caption{Evolution of the weight vectors for EXP3, $\eta = 1$ and $T = 1000$, as a function of $t$.}
  \label{fig:sub2}
\end{subfigure}%
\hspace{.5mm}
\begin{subfigure}{.28\textwidth}
  \centering
  \includegraphics[height=100pt]{image1/EXP3/EXP3_comparison.png}
  \caption{Average loss for EXP3 over 10 simulations, for $\eta=1$ and $T=1000$, as a function of t}
  \label{fig:sub1}
\end{subfigure}%
\caption{}
\label{fig:cum-regrets}
\end{figure}

We observe the same behavior for the weight vectors and the average loss as for the EWA algorithm. The cumulative regret is again under the optimal bound even if the $\eta$ value is not the optimal one.

\begin{figure}[!h]
\begin{center}
\includegraphics[height=190 pt]{image1/EXP3/EXP3_comparison_eta.png} 
\end{center}
\caption{Average loss for EXP3 as a function of t, with $\eta$ varying on a grid, and $T=1000$.}
\end{figure}

This comparison highlights the differences between EWA and EXP3. Indeed, the best cumulative regret seems to be much higher than on figure 6 for EWA, and the value of $\eta$ more discriminant. Even if the best $\eta$ is again around 0.3, small variations around this values induces big differences in the cumulative regret. Thus, EXP3 is less stable than EWA when considering the optimal $\eta$ value.
    \end{solution}

    \item Repeat Question 4.a) and 4.b) with \lstinline{EXP3} instead of \lstinline{EWA}.
    
    \begin{solution}
    As previously, we can obtain a loss of $0$, but \lstinline{EXP3} is less stable than \lstinline{EWA}. Indeed, although we have a loss of $0$ as shown in figure \ref{fig:q7a}, the strategy obtained is not the uniform distribution but a constant one: both the player and the adversary always choose the same action.
    
    \begin{figure}[h!]
    \centering
    \begin{subfigure}{.47\textwidth}
      \centering
      \includegraphics[height=150pt]{image1/q7a_average_loss.png}
      \caption{Average loss as a function of $t$ when playing \lstinline{EXP3} vs \lstinline{EXP3}.}
      \label{fig:q7a}
    \end{subfigure}%
    \hspace{2mm}
    \begin{subfigure}{.47\textwidth}
      \centering
      \includegraphics[height=150pt]{image1/q7b_norm_pt.png}
      \caption{Distance between the player's distribution and the uniform distribution.}
      \label{fig:q7b}
    \end{subfigure}%
    \caption{}
    \end{figure}
    
    As a result, when we plot the distance between the player's distribution and the uniform distribution, it does not tend to $0$, as shown in figure \ref{fig:q7b}.

    \end{solution}
\end{enumerate}

\newpage
\paragraph{Optional extentions} The following questions are optional.

\begin{enumerate}[resume]
    \item Repeat Question 4.a) when the adversary is playing a \lstinline{UCB} algorithm. Who wins between \lstinline{UCB} and \lstinline{EXP3}?
    
    \begin{solution}

    \end{solution}
    
    
    \item In the last lecture, we see that EXP3 has a sublinear expected regret. Yet, as shown by question 6.e), it is extremely unstable with a large variance. Implement \lstinline{EXP3.IX} (see Chapter 12 of \cite{lattimore2018bandit}) a modification of \lstinline{EXP3} that controls the regret in expectation and simultaneously keeps it stable. 
    Repeat question 3.e) with \lstinline{EXP3.IX}.
    
    \begin{solution}

    \end{solution}
    
    
    \item Try different games (not necessarily zero-sum games). In particular, how these algorithms behave for the prisoner's dilemna (see wikipedia)? The prisoner's dilemna is a two-player games that shows why two completely rational individuals might not cooperate, even if it appears that it is in their best interests to do so. The losses matrices are:
    \begin{equation*}
    L^{(player)} = \begin{pmatrix}
       1 & 3 \\
       0 & 2 
    \end{pmatrix} \quad \text{and} \quad 
    L^{(adversary)} = \begin{pmatrix}
       1 & 0 \\
       3 & 2 
    \end{pmatrix}  \,.
    \end{equation*}
    
    \begin{solution}

    \end{solution}
\end{enumerate}