\documentclass[a4paper, 11pt, twocolumn, landscape]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{amssymb}
\usepackage{amsthm, thmtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{stmaryrd}
\usepackage{dsfont}
\usepackage[all]{xy}

\usepackage{enumitem}

\usepackage{nameref, hyperref, cleveref}
	\hypersetup{
		pdfnewwindow=false,
		colorlinks=true,
		linkcolor=black,
		filecolor=black,
		urlcolor=blue,
		}

\usepackage{multicol}
	\setlength{\columnseprule}{0.1pt}

\usepackage[top=2cm, bottom=2cm, left=1cm, right=1cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{MVA Fall 2019}
\fancyhead[C]{Convex Optimization}
\fancyhead[R]{ENS Ulm}

\title{Sheet: Convex Optimization and applications in Machine Learning}
\author{Antoine Moulin}
\date{}

\usepackage{titletoc}
\usepackage{titlesec}
% 	\titleformat{name = \section, numberless}[display]{\normalfont\Large\filcenter}{\vspace{-1cm}\titlerule[1pt]\vspace{1pt}\titlerule}{-0.5pc}{\LARGE\filcenter}[\vspace{1pc}\titlerule]
%	\titleformat{name = \section}[display]{\normalfont\Large\filcenter}{\vspace{-0.5cm}\titlerule[1pt]\vspace{1pt}\titlerule}{-0.5pc}{\Large\filcenter}[\vspace{0.5pc}\titlerule]
%	\titleformat{name = \section, numberless}[display]{\normalfont\Large\filcenter}{\vspace{-0.5cm}\titlerule[1pt]\vspace{1pt}\titlerule}{-0.5pc}{\Large\filcenter}[\vspace{0.5pc}\titlerule]
%	\titlespacing{\section}{0pt}{0pt}{*5}

\allowdisplaybreaks
\jot=0pt

\usepackage[dvipsnames]{xcolor}

\declaretheoremstyle[
    spaceabove=3pt, 
    spacebelow=4pt, 
    headfont=\normalfont\bfseries,
    notefont=\color{MidnightBlue} \bfseries,
    postheadspace=.5em,
    notebraces={}{},
    name={\ignorespaces},
    numbered=no,
    headpunct={\color{MidnightBlue} $\,-$}]{def-style}
\declaretheorem[refname = {definition, définitions}, Refname = {Definition, Definitions}, style=def-style]{definition}

\declaretheorem[numbered = yes, name = Example, refname = {example, examples}, Refname = {Example, Examples}]{example}

\newtheoremstyle{lemma-style}%
{3pt}% Space above
{4pt}% Space below 
{\itshape}% Body font
{}% Indent amount
{\normalfont\bfseries\color{CarnationPink}}% Theorem head font
{\color{CarnationPink} $\,-$}% Punctuation after theorem head
{.5em}% Space after theorem head
{\thmname{\@ifempty{#3}{#1}\@ifnotempty{#3}{#3}}}% Theorem head spec (can be left empty, meaning ‘normal’)

\theoremstyle{lemma-style}
\newtheorem{lemma}{Lemma}


\newtheoremstyle{thm-style}%
{3pt}% Space above
{4pt}% Space below 
{\itshape}% Body font
{}% Indent amount
{\normalfont\bfseries\color{Mahogany}}% Theorem head font
{\color{Mahogany} $\,-$}% Punctuation after theorem head
{.5em}% Space after theorem head
{\thmname{\@ifempty{#3}{#1}\@ifnotempty{#3}{#3}}}% Theorem head spec (can be left empty, meaning ‘normal’)

\theoremstyle{thm-style}
\newtheorem{theorem}{Theorem}


\newtheoremstyle{prop-style}%
{3pt}% Space above
{4pt}% Space below 
{\itshape}% Body font
{}% Indent amount
{\normalfont\bfseries\color{Dandelion}}% Theorem head font
{\color{Dandelion} $\,-$}% Punctuation after theorem head
{.5em}% Space after theorem head
{\thmname{\@ifempty{#3}{#1}\@ifnotempty{#3}{#3}}}% Theorem head spec (can be left empty, meaning ‘normal’)

\theoremstyle{prop-style}
\newtheorem{proposition}{Proposition}


\newtheoremstyle{cor-style}%
{3pt}% Space above
{4pt}% Space below 
{\itshape}% Body font
{}% Indent amount
{\normalfont\bfseries\color{BurntOrange}}% Theorem head font
{\color{BurntOrange} $\,-$}% Punctuation after theorem head
{.5em}% Space after theorem head
{\thmname{\@ifempty{#3}{#1}\@ifnotempty{#3}{#3}}}% Theorem head spec (can be left empty, meaning ‘normal’)

\theoremstyle{cor-style}
\newtheorem{corollary}{Corollary}




\declaretheoremstyle[
    spaceabove=3pt, 
    spacebelow=4pt, 
    headfont=\color{OliveGreen}\normalfont\bfseries,
    notefont=\color{OliveGreen}\bfseries,
    postheadspace=.5em,
    name={\ignorespaces},
    numbered=no,
    headpunct={\color{OliveGreen} $\,-$}]{note-style}
\declaretheorem[numbered = no, name = Note, refname = {note, notes}, Refname = {Note, Notes}, style=note-style]{note}



\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\mipi}{[- \infty, + \infty]}

\newcommand{\Rn}{\mathbb{R}^{n}}
\newcommand{\Rp}{\mathbb{R}^{p}}
\newcommand{\Rq}{\mathbb{R}^{q}}

\renewcommand{\L}{L^{1}(\R)}
\newcommand{\Ld}{L^{2}(\R)}
\newcommand{\Lp}{L^{p}}
\newcommand{\Linf}{L^{\infty}(\R)}

\newcommand{\Sn}{\mathbf{S}^{n}}

\newcommand{\ps}[2]{#1 \overset{p.s.}{\longrightarrow}#2}
\newcommand{\prob}[2]{#1 \overset{\P}{\longrightarrow}#2}
\newcommand{\lp}[2]{#1 \overset{\Lp}{\longrightarrow}#2}
\newcommand{\law}[2]{#1 \Longrightarrow #2}
\newcommand{\dist}[2]{#1 \overset{d}{\longrightarrow}#2}

\newcommand{\La}{L^{1}}
\newcommand{\Lda}{L^{2}}
\newcommand{\Lpa}{L^{p}}
\newcommand{\Linfa}{L^{\infty}}

\renewcommand{\l}{\ell^{1}(\Z)}
\newcommand{\ld}{\ell^{2}(\Z)}
\newcommand{\linf}{\ell^{\infty}(\Z)}

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\relint}{relint}
\DeclareMathOperator{\prox}{prox}

\renewcommand{\emph}[1]{\textbf{\textit{#1}}}

\newcommand{\rline}{\vspace{0em}\noindent\rule{\textwidth}{1pt}\vspace{0em}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\grad}[1]{\nabla{#1}}
\newcommand{\gradwrt}[2]{\nabla_#1{#2}}
\newcommand{\hess}[1]{\nabla^2{#1}}
\newcommand{\set}[2]{\{ #1 \hspace{.1em} | \hspace{.1em}#2 \}}
\newcommand{\Tr}[1]{\text{Tr}\hspace{0.2em}#1}

\begin{document}
\begin{small}
%\maketitle	
%\tableofcontents



\newpage
\section{Convex sets}
\vspace{-7pt}
	\subsection{Definitions}
\vspace{-7pt}

\emph{Convex hull} of $S$: $\conv S = \left\{ \sum_{i = 1}^{k} \theta_{i} x_{i}: k \in [\![ 1, |S| ]\!], \forall i, x_{i} \in S, \theta \succeq 0, \mathbf{1}_{k}^{T} \theta = 1 \right\}$. \\
\emph{Hyperplane}: $\left\{ x: a^{T}x = b \right\}$ (affine). \emph{Halfspace}: $\left\{ x: a^{T}x \leq b \right\}$ (convex). \\
\emph{Euclidean ball} (or ball) in $\Rn$: $B(x_{c}, r) = \left\{ x: ||x - x_{c}||_{2} \leq r \right\}, r > 0$ (convex). \\
\emph{Ellipsoid}: $\mathcal{E} = \left\{ x: (x - x_{c})^{T} P^{-1} (x - x_{c}) \leq 1 \right\}$
where $P \in \mathcal{S}_{n}^{++}$. Another representation is $\left\{ x_{c} + Au: ||u||_{2} \leq 1 \right\}$ with $A$ square and nonsingular (convex). \\
\emph{Norm cone}: $C = \left\{ (x, t): ||x|| \leq t \right\} \subset \mathbb{R}^{n+1}$. $\|\cdot\|_{2}$: second-order cone (convex). \\
\emph{Polyhedron}: $\mathcal{P} = \left\{ x: Ax \preceq b, Cx = d \right\}$ (convex).

\vspace{-10pt}
	\subsection{Operations that preserve convexity}

Show convexity of $C$: (i) definition, (ii) operations that preserve convexity. \\
\emph{Intersection}: the intersection of (any number of) convex sets is convex. \\
\emph{Affine functions}: the image or inverse image by an affine function preserves convexity (e.g. scaling, translation, projection). \\
\emph{Perspective function}: $P: \mathbb{R}^{n+1} \rightarrow \Rn$, $\dom P = \Rn \times \R_{+}^{*}$, $P(z, t) = z / t$. If $C \subset \dom P$ convex, then $P(C)$ convex. If $C \subset \Rn$ convex, then $P^{-1}(C)$ convex. \\
\emph{Linear-fractional function}: compose the perspective function with an affine function, i.e. $f(x) = \frac{Ax + b}{c^{T}x + d}$, $\dom f = \left\{ x: c^{T}x + d > 0 \right\}$ (same results about convexity).

\vspace{-10pt}
	\subsection{Separating and supporting hyperplanes}

\textbf{Separating hyperplane theorem}. $C$, $D$ nonempty disjoint convex sets. Then there exist $a \neq 0$ and $b$ such that $a^{T}x \leq b$ for all $x \in C$ and $a^{T}x \geq b$ for all $x \in D$. \\
\textbf{Definition}. The \emph{supporting hyperplane} to set $C$ at a boundary point $x_{0}$ is defined by $\left\{ x: a^{T}x = a^{T}x_{0} \right\}$ where $a \neq 0$ and for all $x \in C$, $a^{T}x \leq a^{T}x_{0}$. \\
\textbf{Supporting hyperplane theorem}. $C$ convex. Then there exists a supporting hyperplane at every boundary point of $C$.

\vspace{-10pt}
	\subsection{Dual cones and generalized inequalities}

\emph{Proper cone}: $K \subset \Rn$ convex, closed, solid ($\mathbf{int}\, K \neq \emptyset$), pointed (contains no line i.e., if $x, -x \in K$, then $x = 0$)). \emph{Generalized inequality}, $x \preceq_{K} y$ iff $y - x \in K$. \\
\emph{Dual cone}. $K$ a cone. $K^{*} = \left\{ y: \forall x \in K, x^{T}y \geq 0 \right\}$ is the \emph{dual cone} of $K$. $K^{*}$ is a cone, and is always convex. Geometrically, $y \in K^{*}$ if and only if $-y$ is the normal of a hyperplane that supports $K$ at the origin. \\
\textbf{Properties}. $K, K_{1}, K_{2}$ cones. (i) $K^{*}$ closed and convex, (ii) $K_{1} \subset K_{2}$ implies $K_{2}^{*} \subset K_{1}^{*}$, (iii) if $\mathbf{int} K \neq \emptyset$, then $K^{*}$ pointed, (iv) if $\overline{K}$ is pointed then $\mathbf{int} K^{*} \neq \emptyset$, (v) $K^{**} = \overline{\conv K}$ (hence if $K$ is convex and closed, $K^{**} = K$). These properties show that if $K$ is a proper cone, then so is its dual $K^{*}$, and moreover, that $K^{**} = K$.

\section{Convex optimization problems}
    \subsection{Basic properties and examples}

\textit{Convex}: $ax + b$, $e^{ax}$, $x^{\alpha}$ ($\alpha \in \R \setminus (0, 1)$), $|x|^p$ ($p \geq 1$), $x \log x$, norms (e.g. $\lambda_{\max}(X^TX)^{1/2}$). \\
\textit{Concave}: $ax + b$, $x^{\alpha}$ on $\R_{++}$ ($\alpha \in [0, 1]$), $\log x$ on $\R_{++}$. \\
\textbf{Property}. $f: \Rn \rightarrow \R$ convex iff $t \mapsto f(x + tv)$ convex for any $x \in \dom f, v \in \Rn$. \\
\textbf{1st-order cond.}: diff. $f$ with convex domain is convex iff $f(y) \geq f(x) + \nabla f(x)^T(y-x)$. \\
\textbf{2nd-order cond.}: twice diff. $f$ with convex domain is convex iff $\nabla^2 f(x) \succeq 0$. \\
\emph{$\alpha$-sublevel set}: $C_{\alpha} = \left\{ x \in \dom f, f(x) \leq \alpha \right\}$ (convex if $f$ convex).

    \subsection{Operations that preserve convexity}

Establishing convexity: (i) definition, (ii) Hessian, (iii) operations that preserve convexity. \\
\textbf{Nonnegative weighted sum, composition with affine function} (e.g. log barrier), \textbf{pointwise maximum/supremum, composition} ($h \circ g$ convex if $g, h$ convex, $\Tilde{h}$ nondecreasing or if $g$ concave, $h$ convex, $\Tilde{h}$ nonincreasing), \textbf{minimization on a convex set} ($\inf_y x^TAx + 2x^TBy + y^TCy = x^T(A-BC^{-1}B^T)x$), \textbf{perspective} ($g(x, t) = t f(x/t)$).

    \subsection{Definitions}

\emph{Conjugate function}: $f^{*}(y) = \sup_{x} \left( y^Tx - f(x) \right)$ convex. \\
\emph{Quasiconvex}: if sublevel sets are convex. \textbf{Jensen}: $f(\theta x + (1-\theta) y) \leq \max \left\{ f(x), f(y) \right\}$. \\
\emph{Log-concave}: $f( \theta x + (1-\theta)y) \geq f(x)^{\theta}f(y)^{1-\theta}$ (e.g. $x^a, a \geq 0$, normal density, CDF Gaussian). Twice diff. $f$ log-concave iff $f(x) \nabla^2 f(x) \preceq \nabla f(x) \nabla f(x)^T$.

    \subsection{Convex optimization problem}

Feasible set of a convex optimization problem is convex.

\begin{multicols}{2}
  \emph{\noindent Linear program (LP)}:
  \begin{displaymath}
    \begin{array}{ll}
      \min_{x} & c^T x + d\\
      \text{s.t.} & G x \preceq h \\
      & A x = b
    \end{array}
  \end{displaymath}

  \emph{\noindent Quadratic program (QP)}:
  \begin{displaymath}
    \begin{array}{ll}
      \min_{x} & \frac{1}{2} x^T P x + q^T x + r\\
      \text{s.t.} & G x \preceq h \\
      & A x = b
    \end{array}
  \end{displaymath}

  \emph{\noindent Quadratically constrained quadratic program (QCQP)}:
  
  \begin{displaymath}
    \begin{array}{ll}
      \min_{x} & \frac{1}{2} x^T P x + q^T x + r \\
      \text{s.t.} & \frac{1}{2} x^T P_i x + q_i^T x + r_i \preceq 0, A x = b
    \end{array}
  \end{displaymath}

  \emph{\noindent Second order cone program (SOCP)}:
  \begin{displaymath}
    \begin{array}{lll}
      \min_{x} & f^T x &\\
      \text{s.t.} & \|A_i x + b_i\|_2 \leq c_i^T x + d_i, F x = g
    \end{array}
  \end{displaymath}
\end{multicols}

\noindent \emph{Chebyshev center:} of $\mathcal{P} = \left\{ x, a_i^T x \leq b_i, i = 1, \dots, m \right\}$ is center of largest inscribed ball $\mathcal{B} = \left\{ x_c + u, \|u\|_2 \leq r \right\}$. $a_i^T x \leq b_i$ for all $x \in \mathcal{B}$ iff $\sup \left\{ a_i^T \left( x_c + u \right), \|u\|_2 \leq r \right\} = a_i^T x_c + r \|a_i\|_2 \leq b_i$. $x_c, r$ determined by solving the LP: $\max r$ s.t. $a_i^T x_c + r \|a_i\|_2 \leq b_i$. \\
\emph{Perron-Frobenius eigenvalue}: exists for (elementwise) positive square $A$. Real positive eigenvalue of $A$, equal to $\max_i |\lambda_i(A)|$. $A^k \sim \lambda_{pf}^k$. \\
\textbf{Properties}. $\lambda_{\max}(A) \leq t$ iff $A \preceq tI$. $\|A\|_2 \leq t$ iff $A^TA \preceq t^2I$.

\section{Duality}
    \subsection{Theory}

\emph{Lagrangian}: ($f_i \leq 0$, $h_i = 0$). $L(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^{m} \lambda_i f_i(x) + \sum_{i=1}^{p} \nu_i h_i(x)$. \\
\emph{Lagrange dual function}: $g(\lambda, \nu) = \inf_{x} L(x, \lambda, \nu)$ (concave). If $\lambda \succeq 0, g(\lambda, \nu) \leq p^*$. \\
\emph{Dual norm}: $\|v\|_* = \sup_{\|u\| \leq 1} u^Tv$. $\|y\|^* = 0$ if $\|y\|_* \leq 1$, $-\infty$ otherwise. \\
\emph{Lagrange dual problem}: maximize $g(\lambda, \nu)$ s.t. $\lambda \succeq 0$. \\
\emph{Slater's condition}: for a convex problem, if $\exists x \in \relint{\mathcal{D}},\ s.t.\, f_i(x) < 0,\ ,\ A x = b$ (strict feasibility), then strong duality holds (can be relaxed to $f_i(x) \le 0$ for affine constraints) and dual optimal value is attained. \\
\emph{Complementary slackness}: suppose $x^*$ primal optimal, $(\lambda^*$, $\nu^*)$ dual optimal, then $x^*$ minimizes $L(x, \lambda^*, \nu^*)$ over $x$ and $\lambda_i^* f_i(x^*) = 0,\ i = 1,\dots, m$. \\
\emph{KKT conditions}: suppose $f_i$ and $h_i$ are differentiable, let $x^*$ primal optimal, $(\lambda^*$, $\nu^*)$ dual optimal, then \begin{itemize}
\item $\nabla f_0 (x^*) + \sum_{i = 1}^m \lambda_i^* \nabla f_i(x^*) + \sum_{i = 1}^m \nu_i^* \nabla h_i(x^*) = 0$ (first-order condition).
\item $f_i(x^*) \le 0,\ i = 1, \dots, m$ (primal feasibility).
\item $h_i(x^*) = 0,\ i = 1, \dots, m$ (primal feasibility).
\item $\lambda_i^* \geq 0,\ i = 1, \dots, m$ (dual feasibility).
\item $\lambda_i^* f_i(x^*) = 0,\ i = 1, \dots, m$ (complementary slackness).
\end{itemize}

\noindent \emph{KKT conditions for convex problem}: KKT condition are also sufficient if the problem is convex, e.g, if $(\tilde{x}, \tilde{\lambda}, \tilde{\nu})$ satisfy the KKT conditions, then $\tilde{x}$ is primal optimal, $(\tilde{\lambda}, \tilde{\nu})$ is dual optimal and we have $0$
duality gap. \\
\emph{KKT with Slater's condition} : if Slater's condition holds, x is optimal i.i.f there are $(\lambda, \nu)$ s.t $(x, \lambda, \nu)$ satisfy the KKT conditions.

\subsection{Examples}

\begin{equation*}
\begin{array}{l|l|l}
    \begin{array}{lll}
      \underset{x}{\text{min}} & c^T x \\
      \text{subject to} & A x = b \\
                               &  x \succeq 0 \\
      & & \\
      \underset{x}{\text{max}} & - b^T \mu \\
      \text{subject to} & A^T \mu + c \succeq 0\\
    \end{array} &
    \begin{array}{lll}
      \underset{x}{\text{min}} & c^T x \\
      \text{subject to} & A x \preceq b \\
      & & \\
      \underset{x}{\text{max}} & - b^T \lambda \\
      \text{subject to} & A^T \lambda + c = 0\\
                               & \lambda \succeq 0\\
    \end{array} &
    \begin{array}{lll}
      \underset{x}{\text{min}} & \norm{x} \\
      \text{subject to} & A x = b \\
      & & \\
      \underset{x}{\text{max}} & - b^T \mu \\
      \text{subject to} & \norm{A^T \lambda} \le 1\\
    \end{array}
\end{array}
\end{equation*}

\rline
\begin{equation*}
  \begin{array}{l|l}
    \begin{array}{ll}
      \underset{x}{\text{min}} & \log\det{X^{-1}}\\
      \text{subject to} & a_i^T X a_i \le 1,\ i = 1, \dots, m\\
                               & \\
      \underset{x}{\text{max}} & - \log \det(\sum_{i = 1}^m \lambda_i a_i a_i^T) \\
                               & - 1^T \lambda + n\\
      \text{subject to} & \lambda \succeq 0\\
    \end{array} &
                  \begin{array}{lll}
                    \underset{x}{\text{min}} & \frac{1}{2}x^T P_0 x + q_0^T x + r_0 &\\
                    \text{subject to} & \frac{1}{2}x^T P_i x + q_i^T x + r_i \le 0 & \\
                                             & & \\
                    \underset{x}{\text{max}} & - \frac{1}{2} q(\lambda)^T P(\lambda) q(\lambda) + r(\lambda) \\
                    \text{subject to} & \lambda \succeq 0\\
                  \end{array} \\
  \end{array}
\end{equation*}

\section{Extras}

\emph{Dual norm}: $\norm{z}_* = \sup \set{z^T x}{\norm{x} \le 1}$. $\forall x, z \in \Rn,\ z^T x \le \norm{x} \norm{z}_*$. \\
\emph{Singular values}: $\sigma_{\text{max}}(A) = \sup_{x \ne 0, y \ne 0} \frac{x^T A y}{\norm{x}_2 \norm{y}_2}$. \\
\emph{Schur complement}: Let $X \in \Sn$, $X =
\begin{bmatrix}
    A & B \\
    B^T & C
  \end{bmatrix}$ where $A \in \mathcal{S}^k$, If $\det{A} \ne 0$,
  $S = C - B^T A^{-1} B$ is the Schur complement of A in X.
  \begin{itemize}
  \item $\det{X} = \det{A} \det{S}$, \,\,$\bullet\,$ $\inf_{u} \begin{bmatrix}
    u \\
    v
  \end{bmatrix}^T \begin{bmatrix}
    A & B \\
    B^T & C
  \end{bmatrix} \begin{bmatrix}
    u \\
    v
  \end{bmatrix}  = v^T S v$
\item $X \succ 0$ $\iff$ $A \succ 0$ and $S \succ 0$, \,\,$\bullet\,$ if $A \succ 0$ then $X \succeq 0$ $\iff$ $S \succeq 0$
\end{itemize}

\noindent \emph{Taylor's approximation}: $\hat{f}(x + v) = f(x) + \grad{f}(x)^T v + \frac{1}{2} v^T \hess{f} v$. \\
\emph{Newton's method}: $x_{n+1} \leftarrow x_n - \alpha_n(\hess{f}(x))^{-1} \grad{f}(x)$. \\
\emph{Some Gradients}:
\begin{itemize}
\item $\gradwrt{x}{(a^T x + b)} = a$, \,\,$\bullet\,$ $\gradwrt{x}{(\frac{1}{2}x^T A x)} = \frac{1}{2} (A^T + A) x$, \,\,$\bullet\,$ $\gradwrt{x}{(\Tr(A^T X + b)} = A$.
\item $\gradwrt{x}{(\det(X))} = \bar{X}$, $\bar{X}$ comatrix of $X$ ($\bar{X} = \det(X)X^{-T}$).
\item $\gradwrt{x}{(\log\det(X))} = X^{-1}$, \,\,$\bullet\,$ $f(X) = X^{-1} \Rightarrow \gradwrt{x}{f}(H) = -X^{-1} H X^{-1}$.
\end{itemize}

\end{small}
\end{document}