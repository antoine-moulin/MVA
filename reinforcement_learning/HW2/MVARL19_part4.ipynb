{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MVARL19_part4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsaXz-ENaLbx",
        "colab_type": "text"
      },
      "source": [
        "# Exploration in Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHhiMrobaW8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf mvarl_hands_on/\n",
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git\n",
        "!cd mvarl_hands_on/ && git fetch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jpzHHRYd0pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')\n",
        "import os\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from gridworld import GridWorldWithPits\n",
        "from tqdm import tqdm\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQNxcIZtaSZJ",
        "colab_type": "text"
      },
      "source": [
        "## Finite-Horizon Tabular MDPs\n",
        "We consider finite horizon problems with horizon $H$.For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
        "\n",
        "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
        "\n",
        "\n",
        "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
        "$$\n",
        "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
        "$$\n",
        "where $V_{H+1}(s) = 0$, for any $s$. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Suggestion:\n",
        "- $V$ -> $(H+1, S)$-dimensional matrix\n",
        "- deterministic policy $\\pi$ -> $(H, S)$-dimensional matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smh0opCibDY1",
        "colab_type": "text"
      },
      "source": [
        "**Question 1:** implement backward induction for $V^\\pi$ and $V^\\star$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhLaiao3aR4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(P, R, H, policy):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A,S)-dim matrix\n",
        "            H: horizon\n",
        "            policy: a deterministic policy (H, S)-dim matrix\n",
        "            \n",
        "        Returns:\n",
        "            The V-function of the provided policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    V = np.zeros((H+1, S))\n",
        "\n",
        "    for h in range(H-1, -1, -1):\n",
        "      for s in range(S):\n",
        "        V[h, s] = np.sum(P[s, policy[h, s], :] * (R[s, policy[h, s], :] + V[h+1]))\n",
        "\n",
        "    return V\n",
        "\n",
        "def backward_induction(P, R, H):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A,S)-dim matrix\n",
        "            H: horizon\n",
        "            \n",
        "        Returns:\n",
        "            The optimal V-function\n",
        "            The optimal policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    \n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    V = np.zeros((H+1, S))\n",
        "\n",
        "    for h in range(H-1, -1, -1):\n",
        "      for s in range(S):\n",
        "        expected_rewards = np.sum( P[s] * (R[s] + V[h+1]), axis=1)\n",
        "        policy[h, s] = np.argmax(expected_rewards)\n",
        "        V[h, s] = np.sum(P[s, policy[h, s], :] * (R[s, policy[h, s], :] + V[h+1]))\n",
        "\n",
        "    return V, policy    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_l6lqhDbYmQ",
        "colab_type": "text"
      },
      "source": [
        "Let's set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkOs-0y_bd61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)\n",
        "H = 6\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Paggla09bl6S",
        "colab_type": "text"
      },
      "source": [
        "We should test previous functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgeanS2NbpQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "V, optimal_pol = backward_induction(env.P, env.R, H)\n",
        "print(V)\n",
        "Vpi = evaluate_policy(env.P, env.R, H, optimal_pol)\n",
        "print(Vpi)\n",
        "assert np.allclose(V, Vpi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MACThGeMb48-",
        "colab_type": "text"
      },
      "source": [
        "Run the policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbyoa9Qpb6Yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(H):\n",
        "    next_state, reward, _, _ = env.step(optimal_pol[i, state])\n",
        "    env.render()\n",
        "    state = next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZtdzpwUcHYj",
        "colab_type": "text"
      },
      "source": [
        "Finally we are ready to implement our exploration algorithm.\n",
        "\n",
        "**Question 2**: implement the **UCB-VI** algorithm.\n",
        "\n",
        "UCBVI is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
        "In this setting, the regret is defined as\n",
        "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
        "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
        "\n",
        "The structure of the algorithm is as follow\n",
        "\n",
        "For $k = 1, \\ldots, K$ do<br>\n",
        "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
        "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
        ">> for $h=1, \\ldots, H$<br>\n",
        ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
        ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
        ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
        "\n",
        "<font color='#ed7d31'>Optimistic planning</font>\n",
        "\n",
        "UCBVI exploits exploration bonus to perform optimistic planning on the empirical MDP $(\\hat{p}, \\hat{r})$.\n",
        "The optimal Q-function of this MDP can be obtained using backward induction.\n",
        "\n",
        "The optimal Bellman operator for Q-function is defined similarly as\n",
        "$$\n",
        "Q_h^\\star(s,a) =  b(s,a) + \\sum_{s'} p(s'|s,a) \\left( r(s,a,s') + \\max_{a'} Q_{h+1}^\\star(s',a')\\right) \n",
        "$$\n",
        "where $Q_{H+1}(s,a) = 0$ and $b$ is an exploration bonus.\n",
        "\n",
        "<font color='#ed7d31'>Exploration Bonus</font>\n",
        "\n",
        "Using Hoeffding's bound we have that\n",
        "$$\n",
        "b_{k,h}(s,a) = 7(H-h+1)L\\sqrt{\\frac{1}{N_k(s,a)}}\n",
        "$$\n",
        "where $L = \\ln(5SAT/\\delta)$.\n",
        "\n",
        "A tighter exploration bonus is obtained using Bernstein's bound. Since it's expression is much more complicated, we provided the code (see `compute_bernstein_bonus`).\n",
        "\n",
        "\n",
        "Refer to [Minimax Regret Bounds for Reinforcement Learning](https://arxiv.org/abs/1703.05449) for additional details.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMJTmLPcOwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UCBVI:\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        np.random.seed(seed=config['seed'])\n",
        "        self.env = config['env']\n",
        "        self.horizon = config['horizon']\n",
        "        self.scale_factor = config['scale_factor']\n",
        "        self.nb_repetitions = config['repetitions']\n",
        "        self.nb_episodes = config['episodes']\n",
        "        assert config['b_type'] in ['hoeffding', 'bernstein']\n",
        "        self.b_type = config['b_type']\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        S, A = self.env.Ns, self.env.Na\n",
        "        self.delta = 0.1\n",
        "        self.t = 0\n",
        "        self.episode = 0\n",
        "        self.episode_value = []\n",
        "        self.Phat = np.zeros((S, A, S))\n",
        "        self.Rhat = np.zeros((S, A, S))\n",
        "        self.N_sa = np.zeros((S, A), dtype=np.int)\n",
        "        self.N_sas = np.zeros((S, A, S), dtype=np.int)\n",
        "        self.policy = np.zeros((self.horizon, S), dtype=np.int)\n",
        "        self.V = np.zeros((self.horizon+1, S))\n",
        "        self.Q = np.zeros((self.horizon+1, S, A))\n",
        "        self.bonus = np.zeros((self.horizon, S, A))\n",
        "        \n",
        "    def get_optimistic_q(self):\n",
        "        \"\"\" Compute optimistic Q-function and associated greedy policy\n",
        "        \"\"\"\n",
        "        H = self.horizon\n",
        "        S, A = self.N_sa.shape\n",
        "        self.V.fill(0)\n",
        "        self.Q.fill(0)\n",
        "        for h in reversed(range(H)):\n",
        "          if self.b_type == 'hoeffding':\n",
        "            self.compute_hoeffding_bonus(h)\n",
        "          elif self.b_type == 'bernstein':\n",
        "            self.compute_bernstein_bonus(h, self.V[h+1])\n",
        "\n",
        "          N_sa_zero = np.argwhere(self.N_sa == 0)\n",
        "          N_sa_nzero = np.argwhere(self.N_sa > 0)\n",
        "\n",
        "          if len(N_sa_nzero) > 0:\n",
        "            nz_rows = N_sa_nzero[:, 0]\n",
        "            nz_cols = N_sa_nzero[:, 1]\n",
        "            P_nz = self.Phat[nz_rows, nz_cols]\n",
        "\n",
        "            self.Q[h, nz_rows, nz_cols] = self.bonus[h, nz_rows, nz_cols] + \\\n",
        "                                          np.sum(P_nz * self.Rhat[nz_rows, nz_cols], axis=1) + \\\n",
        "                                          P_nz @ np.max(self.Q[h+1], axis=1)\n",
        "          \n",
        "          self.Q[h, N_sa_zero[:, 0], N_sa_zero[:, 1]] = H\n",
        "\n",
        "          self.policy[h] = np.argmax(self.Q[h], axis=1)\n",
        "          self.V[h] = np.array(list(map(lambda s: self.Q[h, s, self.policy[h, s]], np.arange(S))))\n",
        "          self.V[h] = np.minimum(H - h + 2, self.V[h])\n",
        "                \n",
        "    def compute_hoeffding_bonus(self, h):\n",
        "        \"\"\"Compute Hoeffding-based exploration bonus\n",
        "        \"\"\"\n",
        "        S, A = self.N_sa.shape\n",
        "        L = np.log(5 * S * A * self.nb_episodes * self.horizon / self.delta)\n",
        "        self.bonus[h] = 7 * (self.horizon - h + 1) * L / np.sqrt(np.maximum(1, self.N_sa))\n",
        "        self.bonus[h] *= self.scale_factor\n",
        "        \n",
        "    def compute_bernstein_bonus(self, h, Vhp1):\n",
        "        \"\"\"Compute Bernstein-based exploration bonus\n",
        "\n",
        "        Parameters:\n",
        "            h: state\n",
        "            Vhp1: value function at state h+1 (S-dim vector)\n",
        "        \"\"\"\n",
        "        S, A = self.N_sa.shape\n",
        "        H = self.horizon\n",
        "        n = np.maximum(1, self.N_sa)\n",
        "        L = np.log(5 * S * A * n / self.delta)\n",
        "\n",
        "        mean = self.Phat @ Vhp1\n",
        "\n",
        "        f_var = lambda x: self.Phat[x[0], x[1]] @ (Vhp1 - mean[x[0], x[1]]) ** 2\n",
        "        var = np.array(list(map(f_var, list(np.ndindex(S, A)))))\n",
        "        var = var.reshape((S, A))\n",
        "\n",
        "        T1 = np.sqrt(8 * L * var / n) + 14 * L * (H - h + 2) / (3*n)\n",
        "        T2 = np.sqrt(8 * (H - h + 2)**2 / n)\n",
        "        self.bonus[h] = self.scale_factor * (T1 + T2)\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"Update the internal statistics\n",
        "        \"\"\"\n",
        "        self.N_sa[state, action] += 1\n",
        "        self.N_sas[state, action, next_state] += 1\n",
        "        self.Phat[state, action, next_state] = self.N_sas[state, action, next_state] / self.N_sa[state, action]\n",
        "        self.Rhat[state, action, next_state] = ((self.N_sas[state, action, next_state] - 1) * self.Rhat[state, action, next_state] + reward) / self.N_sas[state, action, next_state]\n",
        "        \n",
        "    def run_episode(self):\n",
        "        episode_reward = 0\n",
        "        state = self.env.reset()\n",
        "        initial_state = state\n",
        "        self.get_optimistic_q()\n",
        "        \n",
        "        \n",
        "        Vpi = evaluate_policy(self.env.P, self.env.R, self.horizon, self.policy)\n",
        "        self.episode_value.append(Vpi[0, initial_state])\n",
        "        \n",
        "        for h in range(self.horizon):\n",
        "            action = self.policy[h, state]\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            self.update(state, action, reward, next_state)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            self.t += 1\n",
        "        self.episode += 1\n",
        "        return initial_state, Vpi\n",
        "    \n",
        "    def train(self):\n",
        "        # compute true value function (for the regret)\n",
        "        trueV, _ = backward_induction(self.env.P, self.env.R, self.horizon)\n",
        "        regret = np.zeros((self.nb_repetitions, self.nb_episodes+1))\n",
        "        for rep in range(self.nb_repetitions):\n",
        "            self.reset()\n",
        "            old_regret = 0\n",
        "            for k in range(self.nb_episodes):\n",
        "                init_state, Vpi = self.run_episode()\n",
        "                episode_regret = trueV[0, init_state] - Vpi[0, init_state] \n",
        "                old_regret += episode_regret\n",
        "                regret[rep, k+1] = old_regret\n",
        "        return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoSaZ3xOckmd",
        "colab_type": "text"
      },
      "source": [
        "Define the settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjk1TbBock-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'env': env,\n",
        "    'scale_factor': 0.1, # we use a scaling factor in order to increase the convergence speed\n",
        "    'seed': 14,\n",
        "    'horizon': H,\n",
        "    'episodes': 40000,\n",
        "    'repetitions': 5,\n",
        "    'b_type': 'hoeffding' # [hoeffding, bernstein]\n",
        "}\n",
        "\n",
        "print(\"Current config is:\")\n",
        "pprint(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSI7NR50cyFx",
        "colab_type": "text"
      },
      "source": [
        "Run the agent and compare the behaviour with Hoeffding and Bernstein bound.\n",
        "\n",
        "A picture is automatically generated (it will show the regret average regret of the two algorithms)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S3ObC9ncydR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "regret = {}\n",
        "for bound in ['hoeffding', 'bernstein']:\n",
        "    tmp_config = copy.copy(config)\n",
        "    tmp_config['b_type'] = bound\n",
        "    agent = UCBVI(config=tmp_config)\n",
        "    regret[bound] = agent.train()\n",
        "\n",
        "    mean_regret = np.mean(regret[bound], axis=0)\n",
        "    std = np.std(regret[bound], axis=0) / np.sqrt(regret[bound].shape[0])\n",
        "    x = np.arange(regret[bound].shape[1])\n",
        "    plt.plot(x, mean_regret, label=bound)\n",
        "    plt.fill_between(x, mean_regret + 2 * std, mean_regret - 2 * std, alpha=0.15)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}